{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TOKENPOST 속보 크롤링 하기\n",
    "URL = \"https://tokenpost.kr/breaking\"\n",
    "time 함수 출처 : http://yujuwon.tistory.com/entry/%ED%98%84%EC%9E%AC-%EB%82%A0%EC%A7%9C-%EA%B0%80%EC%A0%B8%EC%98%A4%EA%B8%B0\n",
    "코드 출처 : http://yoonpunk.tistory.com/6\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.parse import quote\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "URL = \"https://tokenpost.kr/breaking\"\n",
    "URL2 = \"https://www.blockmedia.co.kr/news/article_list/?gCode=AB100\"\n",
    "news_URL = \"https://www.blockmedia.co.kr\"\n",
    "\n",
    "TARGET_URL_BEFORE_PAGE_NUM = \"http://news.donga.com/search?p=\"\n",
    "TARGET_URL_BEFORE_KEWORD = '&query='\n",
    "TARGET_URL_REST = '&check_news=1&more=1&sorting=1&range=2&search_date='\n",
    "\n",
    "now_time = int(datetime.today().hour)\n",
    "now_day = int(datetime.today().day)\n",
    "# 속보 불러오기\n",
    "def get_link_from_news_title_news1(URL, output_file):\n",
    "        source_code_from_URL = urllib.request.urlopen(URL)\n",
    "        soup = BeautifulSoup(source_code_from_URL, 'lxml',\n",
    "                             from_encoding='utf-8')\n",
    "        for title in soup.find_all('div', 'breakListRight'):\n",
    "            title_link = title.select('a')\n",
    "            article_URL = title_link[0]['href']\n",
    "            get_text_news1(article_URL, output_file)\n",
    "\n",
    "\n",
    "# 기사 본문 내용 긁어오기 (하이퍼 링크 타기 )\n",
    "def get_text_news1(URL, output_file):\n",
    "    source_code_from_url = urllib.request.urlopen(URL)\n",
    "    soup = BeautifulSoup(source_code_from_url, 'lxml', from_encoding='utf-8')\n",
    "    content_of_article = soup.select('div.viewContentArticle')\n",
    "    for item in content_of_article:\n",
    "        string_item = str(item.find_all(text=True))\n",
    "        temp = str(item.find('p','viewInfoTime'))\n",
    "        time = temp[37:39]\n",
    "        if time[0] == '0':\n",
    "            time = time[1]\n",
    "        day = temp[32:35]\n",
    "        if day[0] == '0':\n",
    "            day = day[1]\n",
    "        time = int(time)\n",
    "        day = int(day)\n",
    "        if day == now_day and time + 6 >= now_time:\n",
    "            output_file.write(string_item)\n",
    "\n",
    "\n",
    "# 속보 불러오기\n",
    "def get_link_from_news_title_news2(URL, output_file):\n",
    "        source_code_from_URL = urllib.request.urlopen(URL)\n",
    "        soup = BeautifulSoup(source_code_from_URL, 'lxml',\n",
    "                             from_encoding='utf-8')\n",
    "        for title in soup.find_all('dd', 'txt'):\n",
    "            title_link = title.select('a')\n",
    "            article_URL = news_URL + title_link[0]['href']\n",
    "            #print(article_URL)\n",
    "            get_text_news2(article_URL, output_file)\n",
    "\n",
    "\n",
    "# 기사 본문 내용 긁어오기 (하이퍼 링크 타기 )\n",
    "def get_text_news2(URL, output_file):\n",
    "    source_code_from_url = urllib.request.urlopen(URL)\n",
    "    soup = BeautifulSoup(source_code_from_url, 'lxml', from_encoding='utf-8')\n",
    "    content_of_article = soup.select('div.view')\n",
    "    for item in content_of_article:\n",
    "        string_item = str(item.find_all('p'))\n",
    "        temp = item.find('div','viewinfo')\n",
    "        temp_time = str(temp.select('p'))\n",
    "        day = temp_time[12:14]\n",
    "        time = temp_time[15:17]\n",
    "        if time[0] == '0':\n",
    "            time = time[1]\n",
    "        if day[0] == '0':\n",
    "            day = day[1]\n",
    "        time = int(time)\n",
    "        day = int(day)\n",
    "        if day == now_day and time + 6 >= now_time:\n",
    "            output_file.write(string_item)\n",
    "\n",
    "# 기사 검색 페이지에서 기사 제목에 링크된 기사 본문 주소 받아오기\n",
    "def get_link_from_news_title_news3(page_num, URL, output_file):\n",
    "    for i in range(page_num):\n",
    "        current_page_num = 1 + i*15\n",
    "        position = URL.index('=')\n",
    "        URL_with_page_num = URL[: position+1] + str(current_page_num) \\\n",
    "                            + URL[position+1 :]\n",
    "        source_code_from_URL = urllib.request.urlopen(URL_with_page_num)\n",
    "        soup = BeautifulSoup(source_code_from_URL, 'lxml',\n",
    "                             from_encoding='utf-8')\n",
    "        for title in soup.find_all('p', 'tit'):\n",
    "            title_link = title.select('a')\n",
    "            article_URL = title_link[0]['href']\n",
    "            get_text_news3(article_URL, output_file)\n",
    "\n",
    "\n",
    "# 기사 본문 내용 긁어오기 (위 함수 내부에서 기사 본문 주소 받아 사용되는 함수)\n",
    "def get_text_news3(URL, output_file):\n",
    "    source_code_from_url = urllib.request.urlopen(URL)\n",
    "    soup = BeautifulSoup(source_code_from_url, 'lxml', from_encoding='utf-8')\n",
    "    content_of_article = soup.select('div.article_txt')\n",
    "    time_of_article = soup.select('div.title_foot > span.date01')\n",
    "    temp = str(time_of_article[0])\n",
    "    day = temp[32:34]\n",
    "    time = temp[35:37]\n",
    "    if time[0] == '0':\n",
    "        time = time[1]\n",
    "    if day[0] == '0':\n",
    "        day = day[1]\n",
    "    time = int(time)\n",
    "    day = int(day)\n",
    "    if day == now_day and time + 6 >= now_time:\n",
    "        output_file.write(string_item)\n",
    "    #print(temp_time)\n",
    "    for item in content_of_article:\n",
    "        string_item = str(item.find_all(text=True))\n",
    "        if day == now_day and time + 6 >= now_time:\n",
    "            output_file.write(string_item)\n",
    "\n",
    "\n",
    "# 메인함수\n",
    "def main(argv):\n",
    "    output_file_name = \"result.txt\"\n",
    "    target_URL = URL\n",
    "    output_file = open(output_file_name, 'w',encoding=\"utf-8\")\n",
    "    get_link_from_news_title_news1(target_URL, output_file)\n",
    "    target_URL = URL2\n",
    "    get_link_from_news_title_news2(target_URL, output_file)\n",
    "    keyword = \"비트코인\"\n",
    "    page_num = int(1)\n",
    "    target_URL = TARGET_URL_BEFORE_PAGE_NUM + TARGET_URL_BEFORE_KEWORD \\\n",
    "                 + quote(keyword) + TARGET_URL_REST\n",
    "    get_link_from_news_title_news3(page_num, target_URL, output_file)\n",
    "    output_file.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
